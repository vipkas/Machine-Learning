{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deteksi Emosi Pengguna Twitter\n",
    "\n",
    "Deteksi emosi merupakan salah satu permasalahan yang dihadapi pada ***Natural Language Processing*** (NLP). Alasanya diantaranya adalah kurangnya dataset berlabel untuk mengklasifikasikan emosi berdasarkan data twitter. Selain itu, sifat dari data twitter yang dapat memiliki banyak label emosi (***multi-class***). Manusia memiliki berbagai emosi dan sulit untuk mengumpulkan data yang cukup untuk setiap emosi. Oleh karena itu, masalah ketidakseimbangan kelas akan muncul (***class imbalance***). Pada Ujian Tengah Semester (UTS) kali ini, Anda telah disediakan dataset teks twitter yang sudah memiliki label untuk beberapa kelas emosi. Tugas utama Anda adalah membuat model yang mumpuni untuk kebutuhan klasifikasi emosi berdasarkan teks.\n",
    "\n",
    "### Informasi Data\n",
    "\n",
    "Dataset yang akan digunakan adalah ***tweet_emotion.csv***. Berikut merupakan informasi tentang dataset yang dapat membantu Anda.\n",
    "\n",
    "- Total data: 40000 data\n",
    "- Label emosi: anger, boredom, empty, enthusiasm, fun, happiness, hate, love, neutral, relief, sadness, surprise, worry\n",
    "- Jumlah data untuk setiap label tidak sama (***class imbalance***)\n",
    "- Terdapat 3 kolom = 'tweet_id', 'sentiment', 'content'\n",
    "\n",
    "### Penilaian UTS\n",
    "\n",
    "UTS akan dinilai berdasaarkan 4 proses yang akan Anda lakukan, yaitu pra pengolahan data, ektraksi fitur, pembuatan model machine learning, dan evaluasi.\n",
    "\n",
    "#### Pra Pengolahan Data\n",
    "\n",
    "> **Perhatian**\n",
    "> \n",
    "> Sebelum Anda melakukan sesuatu terhadap data Anda, pastikan data yang Anda miliki sudah \"baik\", bebas dari data yang hilang, menggunakan tipe data yang sesuai, dan sebagainya.\n",
    ">\n",
    "\n",
    "Data tweeter yang ada dapatkan merupakan sebuah data mentah, maka beberapa hal dapat Anda lakukan (namun tidak terbatas pada) yaitu,\n",
    "\n",
    "1. Case Folding\n",
    "2. Tokenizing\n",
    "3. Filtering\n",
    "4. Stemming\n",
    "\n",
    "*CATATAN: PADA DATA TWITTER TERDAPAT *MENTION* (@something) YANG ANDA HARUS TANGANI SEBELUM MASUK KE TAHAP EKSTRAKSI FITUR*\n",
    "\n",
    "#### Ekstrasi Fitur\n",
    "\n",
    "Anda dapat menggunakan beberapa metode, diantaranya\n",
    "\n",
    "1. Bag of Words (Count / TF-IDF)\n",
    "2. N-gram\n",
    "3. dan sebagainya\n",
    "\n",
    "#### Pembuatan Model\n",
    "\n",
    "Anda dibebaskan dalam memilih algoritma klasifikasi. Anda dapat menggunakan algoritma yang telah diajarkan didalam kelas atau yang lain, namun dengan catatan. Berdasarkan asas akuntabilitas pada pengembangan model machine learning, Anda harus dapat menjelaskan bagaimana model Anda dapat menghasilkan nilai tertentu.\n",
    "\n",
    "#### Evaluasi\n",
    "\n",
    "Pada proses evaluasi, minimal Anda harus menggunakan metric akurasi. Akan tetapi Anda juga dapat menambahkan metric lain seperti Recall, Precision, F1-Score, detail Confussion Metric, ataupun Area Under Curve (AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lembar Pengerjaan\n",
    "Lembar pengerjaan dimulai dari cell dibawah ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris: 40000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/tweet_emotions.csv')\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "jml_baris_asli = df.shape[0]\n",
    "print(f'Jumlah baris: {jml_baris_asli}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "##### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris: 39827\n",
      "Jumlah baris duplikasi 173\n"
     ]
    }
   ],
   "source": [
    "# Drop twit yang sama\n",
    "df.drop_duplicates(subset=['content'], inplace=True)\n",
    "\n",
    "# Cek jumlah data\n",
    "jml_baris_drop = df.shape[0]\n",
    "print(f'Jumlah baris: {jml_baris_drop}')\n",
    "print(f'Jumlah baris duplikasi {jml_baris_asli - jml_baris_drop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Mention (@) and URL\n",
    "\n",
    "Remove words begin with @ and URL such as http, https, ftp, ftps, and www"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>Layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>Funeral ceremony   gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends SOON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>We want to trade with someone who has Houston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1956989514</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@sweeetnspicy hiii im on my ipod...i cant fall...</td>\n",
       "      <td>hiii im on my ipod   i cant fall asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1956989526</td>\n",
       "      <td>sadness</td>\n",
       "      <td>dont wanna work 11-830 tomorrow  but i get paid</td>\n",
       "      <td>dont wanna work 11 830 tomorrow  but i get paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1956989560</td>\n",
       "      <td>sadness</td>\n",
       "      <td>feels sad coz i wasnt able to play with the gu...</td>\n",
       "      <td>feels sad coz i wasnt able to play with the gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1956989561</td>\n",
       "      <td>neutral</td>\n",
       "      <td>PrinceCharming</td>\n",
       "      <td>PrinceCharming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1956989601</td>\n",
       "      <td>hate</td>\n",
       "      <td>@ cayogial i wanted to come to BZ this summer ...</td>\n",
       "      <td>cayogial i wanted to come to BZ this summer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id   sentiment                                            content  \\\n",
       "0   1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1   1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2   1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3   1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4   1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "..         ...         ...                                                ...   \n",
       "95  1956989514     sadness  @sweeetnspicy hiii im on my ipod...i cant fall...   \n",
       "96  1956989526     sadness    dont wanna work 11-830 tomorrow  but i get paid   \n",
       "97  1956989560     sadness  feels sad coz i wasnt able to play with the gu...   \n",
       "98  1956989561     neutral                                     PrinceCharming   \n",
       "99  1956989601        hate  @ cayogial i wanted to come to BZ this summer ...   \n",
       "\n",
       "                                        content_clean  \n",
       "0    i know  i was listenin to bad habit earlier a...  \n",
       "1   Layin n bed with a headache  ughhhh   waitin o...  \n",
       "2                 Funeral ceremony   gloomy friday     \n",
       "3                wants to hang out with friends SOON   \n",
       "4    We want to trade with someone who has Houston...  \n",
       "..                                                ...  \n",
       "95            hiii im on my ipod   i cant fall asleep  \n",
       "96    dont wanna work 11 830 tomorrow  but i get paid  \n",
       "97  feels sad coz i wasnt able to play with the gu...  \n",
       "98                                     PrinceCharming  \n",
       "99    cayogial i wanted to come to BZ this summer ...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # python regex lib\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# Membuat kolom baru untuk kebutuhan berbandingan\n",
    "df['content_clean'] = df['content']\n",
    "\n",
    "# Membuat fungsi lambda untuk membuat mention, url\n",
    "rm_rt_url = lambda x: re.sub('(@[A-Za-z0–9\\w]+) | (@\\w+:) | (\\w+:\\/\\/\\S+) | (www.\\S+)',' ',x)\n",
    "rm_punct = lambda x: re.sub('\\W', ' ', x)\n",
    "\n",
    "# Membuat fungsi untuk membuang protocol internet\n",
    "\n",
    "# Map filter\n",
    "df['content_clean'] = df.content_clean.map(rm_rt_url).map(rm_punct)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case Folding\n",
    "\n",
    "Make all twit to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39990</th>\n",
       "      <td>1753918829</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@shonali I think the lesson of the day is not ...</td>\n",
       "      <td>i think the lesson of the day is not to have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39991</th>\n",
       "      <td>1753918846</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@lovelylisaj can you give me the link for the ...</td>\n",
       "      <td>can you give me the link for the kimba diarie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39992</th>\n",
       "      <td>1753918881</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@jasimmo Ooo showing of your French skills!! l...</td>\n",
       "      <td>ooo showing of your french skills   lol thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39993</th>\n",
       "      <td>1753918892</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@sendsome2me haha, yeah. Twitter has many uses...</td>\n",
       "      <td>haha  yeah  twitter has many uses  for me it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>1753918900</td>\n",
       "      <td>happiness</td>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "      <td>succesfully following tayla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "      <td>johnlloydtaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "      <td>happy mothers day  all my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "      <td>happy mother s day to all the mommies out ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
       "      <td>wassup beautiful    follow me    peep out my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "      <td>bullet train from tokyo    the gf and i have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id  sentiment  \\\n",
       "39990  1753918829    neutral   \n",
       "39991  1753918846    neutral   \n",
       "39992  1753918881    neutral   \n",
       "39993  1753918892    neutral   \n",
       "39994  1753918900  happiness   \n",
       "39995  1753918954    neutral   \n",
       "39996  1753919001       love   \n",
       "39997  1753919005       love   \n",
       "39998  1753919043  happiness   \n",
       "39999  1753919049       love   \n",
       "\n",
       "                                                 content  \\\n",
       "39990  @shonali I think the lesson of the day is not ...   \n",
       "39991  @lovelylisaj can you give me the link for the ...   \n",
       "39992  @jasimmo Ooo showing of your French skills!! l...   \n",
       "39993  @sendsome2me haha, yeah. Twitter has many uses...   \n",
       "39994                      Succesfully following Tayla!!   \n",
       "39995                                   @JohnLloydTaylor   \n",
       "39996                     Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out ther...   \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...   \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...   \n",
       "\n",
       "                                           content_clean  \n",
       "39990   i think the lesson of the day is not to have ...  \n",
       "39991   can you give me the link for the kimba diarie...  \n",
       "39992   ooo showing of your french skills   lol thing...  \n",
       "39993   haha  yeah  twitter has many uses  for me it ...  \n",
       "39994                      succesfully following tayla    \n",
       "39995                                    johnlloydtaylor  \n",
       "39996                     happy mothers day  all my love  \n",
       "39997  happy mother s day to all the mommies out ther...  \n",
       "39998   wassup beautiful    follow me    peep out my ...  \n",
       "39999   bullet train from tokyo    the gf and i have ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content_clean'] = df.content_clean.str.lower()\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select Label and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39827,)\n",
      "(39827,)\n",
      "(39827,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df['content_clean'].values\n",
    "y = df['sentiment'].values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Encode Label\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y = encoder.fit_transform(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "Extract TF-IDF with CountVectorizer without Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train Test\n",
    "# Split dulu untuk menghindari leaking information\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Experiment - Just Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_count = count_vect.fit_transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Experiment - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi fitur count (training): 0.5519286902482659\n",
      "Akurasi fitur count (testing): 0.3172231985940246\n",
      "==========\n",
      "Akurasi fitur TF-IDF (training): 0.4586798907755563\n",
      "Akurasi fitur TF-IDF (testing): 0.3024102435350238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## 1st Experiment\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_train_count, y_train)\n",
    "\n",
    "# Predict train\n",
    "y_count_train = mnb.predict(X_train_count)\n",
    "acc_count_train = accuracy_score(y_train, y_count_train)\n",
    "\n",
    "# Predict test\n",
    "y_count_test = mnb.predict(X_test_count)\n",
    "acc_count_test = accuracy_score(y_test, y_count_test)\n",
    "\n",
    "# Print Hasil\n",
    "print(f'Akurasi fitur count (training): {acc_count_train}')\n",
    "print(f'Akurasi fitur count (testing): {acc_count_test}')\n",
    "\n",
    "\n",
    "# =================== #\n",
    "\n",
    "\n",
    "## 2nd Experiment\n",
    "mnb_tf = MultinomialNB()\n",
    "\n",
    "mnb_tf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict train\n",
    "y_tf_train = mnb_tf.predict(X_train_tfidf)\n",
    "acc_tf_train = accuracy_score(y_train, y_tf_train)\n",
    "\n",
    "# Predict test\n",
    "y_tf_test = mnb_tf.predict(X_test_tfidf)\n",
    "acc_tf_test = accuracy_score(y_test, y_tf_test)\n",
    "\n",
    "# Print Hasil\n",
    "print('==========')\n",
    "print(f'Akurasi fitur TF-IDF (training): {acc_tf_train}')\n",
    "print(f'Akurasi fitur TF-IDF (testing): {acc_tf_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi fitur count (training): 0.8027368883588085\n",
      "Akurasi fitur count (testing): 0.32011046949535527\n",
      "==========\n",
      "Akurasi fitur TF-IDF (training): 0.7470889174853269\n",
      "Akurasi fitur TF-IDF (testing): 0.3471001757469244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## 1st Experiment\n",
    "svc = SVC(kernel=\"linear\")\n",
    "\n",
    "svc.fit(X_train_count, y_train)\n",
    "\n",
    "# Predict train\n",
    "y_count_train = svc.predict(X_train_count)\n",
    "acc_count_train = accuracy_score(y_train, y_count_train)\n",
    "\n",
    "# Predict test\n",
    "y_count_test = svc.predict(X_test_count)\n",
    "acc_count_test = accuracy_score(y_test, y_count_test)\n",
    "\n",
    "# Print Hasil\n",
    "print(f'Akurasi fitur count (training): {acc_count_train}')\n",
    "print(f'Akurasi fitur count (testing): {acc_count_test}')\n",
    "\n",
    "\n",
    "# =================== #\n",
    "\n",
    "\n",
    "## 2nd Experiment\n",
    "svc_tf = SVC()\n",
    "\n",
    "svc_tf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict train\n",
    "y_tf_train = svc_tf.predict(X_train_tfidf)\n",
    "acc_tf_train = accuracy_score(y_train, y_tf_train)\n",
    "\n",
    "# Predict test\n",
    "y_tf_test = svc_tf.predict(X_test_tfidf)\n",
    "acc_tf_test = accuracy_score(y_test, y_tf_test)\n",
    "\n",
    "# Print Hasil\n",
    "print('==========')\n",
    "print(f'Akurasi fitur TF-IDF (training): {acc_tf_train}')\n",
    "print(f'Akurasi fitur TF-IDF (testing): {acc_tf_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another Approach\n",
    "\n",
    "Karena hasil kurang oke, maka mungkin kita perlu Stemming atau Lemmatization. Stemming dan Lemmatization adalah sesuatu yang berbeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stemming vs. Lemmatization\n",
    "\n",
    "# Copy DataFrame\n",
    "df_stem = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/afif/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/afif/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "      <td>[i, know, i, was, listenin, to, bad, habit, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony   gloomy friday</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>[wants, to, hang, out, with, friends, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>[we, want, to, trade, with, someone, who, has,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0   i know  i was listenin to bad habit earlier a...   \n",
       "1  layin n bed with a headache  ughhhh   waitin o...   \n",
       "2                funeral ceremony   gloomy friday      \n",
       "3               wants to hang out with friends soon    \n",
       "4   we want to trade with someone who has houston...   \n",
       "\n",
       "                                       content_token  \n",
       "0  [i, know, i, was, listenin, to, bad, habit, ea...  \n",
       "1  [layin, n, bed, with, a, headache, ughhhh, wai...  \n",
       "2                [funeral, ceremony, gloomy, friday]  \n",
       "3        [wants, to, hang, out, with, friends, soon]  \n",
       "4  [we, want, to, trade, with, someone, who, has,...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenized\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_token = TweetTokenizer()\n",
    "df_stem['content_token'] = df_stem['content_clean'].apply(tweet_token.tokenize)\n",
    "\n",
    "df_stem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_token</th>\n",
       "      <th>content_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "      <td>[i, know, i, was, listenin, to, bad, habit, ea...</td>\n",
       "      <td>[i, know, i, was, listenin, to, bad, habit, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n",
       "      <td>[layin, n, bed, with, a, headach, ughhhh, wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony   gloomy friday</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "      <td>[funer, ceremoni, gloomi, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>[wants, to, hang, out, with, friends, soon]</td>\n",
       "      <td>[want, to, hang, out, with, friend, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>[we, want, to, trade, with, someone, who, has,...</td>\n",
       "      <td>[we, want, to, trade, with, someon, who, has, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0   i know  i was listenin to bad habit earlier a...   \n",
       "1  layin n bed with a headache  ughhhh   waitin o...   \n",
       "2                funeral ceremony   gloomy friday      \n",
       "3               wants to hang out with friends soon    \n",
       "4   we want to trade with someone who has houston...   \n",
       "\n",
       "                                       content_token  \\\n",
       "0  [i, know, i, was, listenin, to, bad, habit, ea...   \n",
       "1  [layin, n, bed, with, a, headache, ughhhh, wai...   \n",
       "2                [funeral, ceremony, gloomy, friday]   \n",
       "3        [wants, to, hang, out, with, friends, soon]   \n",
       "4  [we, want, to, trade, with, someone, who, has,...   \n",
       "\n",
       "                                        content_stem  \n",
       "0  [i, know, i, was, listenin, to, bad, habit, ea...  \n",
       "1  [layin, n, bed, with, a, headach, ughhhh, wait...  \n",
       "2                  [funer, ceremoni, gloomi, friday]  \n",
       "3          [want, to, hang, out, with, friend, soon]  \n",
       "4  [we, want, to, trade, with, someon, who, has, ...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemming(text):\n",
    "    stem_text = [stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "\n",
    "df_stem['content_stem'] = df_stem['content_token'].apply(lambda x: stemming(x))\n",
    "\n",
    "df_stem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_token</th>\n",
       "      <th>content_stem</th>\n",
       "      <th>content_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "      <td>[i, know, i, was, listenin, to, bad, habit, ea...</td>\n",
       "      <td>[i, know, i, was, listenin, to, bad, habit, ea...</td>\n",
       "      <td>[i, know, i, wa, listenin, to, bad, habit, ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n",
       "      <td>[layin, n, bed, with, a, headach, ughhhh, wait...</td>\n",
       "      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony   gloomy friday</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "      <td>[funer, ceremoni, gloomi, friday]</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>[wants, to, hang, out, with, friends, soon]</td>\n",
       "      <td>[want, to, hang, out, with, friend, soon]</td>\n",
       "      <td>[want, to, hang, out, with, friend, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>[we, want, to, trade, with, someone, who, has,...</td>\n",
       "      <td>[we, want, to, trade, with, someon, who, has, ...</td>\n",
       "      <td>[we, want, to, trade, with, someone, who, ha, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0   i know  i was listenin to bad habit earlier a...   \n",
       "1  layin n bed with a headache  ughhhh   waitin o...   \n",
       "2                funeral ceremony   gloomy friday      \n",
       "3               wants to hang out with friends soon    \n",
       "4   we want to trade with someone who has houston...   \n",
       "\n",
       "                                       content_token  \\\n",
       "0  [i, know, i, was, listenin, to, bad, habit, ea...   \n",
       "1  [layin, n, bed, with, a, headache, ughhhh, wai...   \n",
       "2                [funeral, ceremony, gloomy, friday]   \n",
       "3        [wants, to, hang, out, with, friends, soon]   \n",
       "4  [we, want, to, trade, with, someone, who, has,...   \n",
       "\n",
       "                                        content_stem  \\\n",
       "0  [i, know, i, was, listenin, to, bad, habit, ea...   \n",
       "1  [layin, n, bed, with, a, headach, ughhhh, wait...   \n",
       "2                  [funer, ceremoni, gloomi, friday]   \n",
       "3          [want, to, hang, out, with, friend, soon]   \n",
       "4  [we, want, to, trade, with, someon, who, has, ...   \n",
       "\n",
       "                                        content_lemm  \n",
       "0  [i, know, i, wa, listenin, to, bad, habit, ear...  \n",
       "1  [layin, n, bed, with, a, headache, ughhhh, wai...  \n",
       "2                [funeral, ceremony, gloomy, friday]  \n",
       "3          [want, to, hang, out, with, friend, soon]  \n",
       "4  [we, want, to, trade, with, someone, who, ha, ...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(text):\n",
    "    lemm_text = [wnl.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "\n",
    "df_stem['content_lemm'] = df_stem['content_token'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "df_stem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select label and Token\n",
    "X_stem = df_stem['content_stem'].values\n",
    "X_lemm = df_stem['content_lemm'].values\n",
    "y = df_stem['sentiment'].values\n",
    "\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train Test\n",
    "X_stem_train, X_stem_test, y_stem_train, y_stem_test = train_test_split(X_stem, y, test_size=0.3, random_state=20)\n",
    "X_lemm_train, X_lemm_test, y_lemm_train, y_lemm_test = train_test_split(X_lemm, y, test_size=0.3, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "\n",
    "\n",
    "count_stem = CountVectorizer(preprocessor=' '.join, stop_words='english')\n",
    "\n",
    "## Stem\n",
    "X_stem_train_count = count_stem.fit_transform(X_stem_train)\n",
    "X_stem_test_count = count_stem.transform(X_stem_test)\n",
    "\n",
    "## Lemm\n",
    "X_lemm_train_count = count_stem.fit_transform(X_lemm_train)\n",
    "X_lemm_test_count = count_stem.transform(X_lemm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5309204390558864\n",
      "0.32036153653025357\n",
      "=======\n",
      "0.5309204390558864\n",
      "0.3187714453092309\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "\n",
    "## Stem\n",
    "mnb_stem = MultinomialNB()\n",
    "\n",
    "mnb_stem.fit(X_stem_train_count, y_stem_train)\n",
    "\n",
    "y_stem_count_train_pred = mnb_stem.predict(X_stem_train_count)\n",
    "acc_stem_count_train = accuracy_score(y_stem_train, y_stem_count_train_pred)\n",
    "print(acc_stem_count_train)\n",
    "\n",
    "y_stem_count_test_pred = mnb_stem.predict(X_stem_test_count)\n",
    "acc_stem_count_test = accuracy_score(y_stem_test, y_stem_count_test_pred)\n",
    "print(acc_stem_count_test)\n",
    "\n",
    "## Lemm\n",
    "mnb_lemm = MultinomialNB()\n",
    "\n",
    "mnb_lemm.fit(X_lemm_train_count, y_lemm_train)\n",
    "\n",
    "y_lemm_count_train_pred = mnb_lemm.predict(X_lemm_train_count)\n",
    "acc_lemm_count_train = accuracy_score(y_lemm_train, y_lemm_count_train_pred)\n",
    "print('=======')\n",
    "print(acc_stem_count_train)\n",
    "\n",
    "y_lemm_count_test_pred = mnb_lemm.predict(X_lemm_test_count)\n",
    "acc_lemm_count_test = accuracy_score(y_lemm_test, y_lemm_count_test_pred)\n",
    "print(acc_lemm_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
